{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Transformer\n",
    "\n",
    "The transformer architecture introduced in the paper [Attention is All You Need](https://arxiv.org/pdf/1706.03762.pdf) has served as the basis for nearly all of the modern day language models.\n",
    "This assumes some understanding of basic deep learning including how CNNs and RNNs work. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How it Works and How to Implement\n",
    "\n",
    "<p align=\"center\">\n",
    "<img src=\"./assets/transformer.png\" height=\"480\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Input/Output Embedding\n",
    "\n",
    "\n",
    "### Positional Encoding\n",
    "\n",
    "\n",
    "### Softmax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Attention Mechanism\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V$$\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./assets/scaled-dot-attention.png\" height=\"360\">\n",
    "</p>\n",
    "\n",
    "### Multihead Attention\n",
    "\n",
    "$$\\begin{align}\n",
    "  \\text{MultiHead}(Q, K, V) &= \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h) W^O\\\\\n",
    "  \\text{where head}_\\text{i} &= \\text{Attention}(QW_i^Q, KW_i^K, VW_i^V)\n",
    "\\end{align}$$\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"./assets/multihead-attention.png\" height=\"360\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder\n",
    "\n",
    "\n",
    "### Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
